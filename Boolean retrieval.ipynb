{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<H1>Importing required libraries</H1>"
      ],
      "metadata": {
        "id": "PghWNF_qVP0B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KoGSfW5ZU7F1"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E54iAYn8VJHj",
        "outputId": "643eb2d4-f4b2-4ddf-89a4-083f966df40f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import logging"
      ],
      "metadata": {
        "id": "BRlXuAhBVjxD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing stopwords and lemmatizer\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "LEMMATIZER = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "zhMeoY2FVnHG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Loading the text files</h1>\n"
      ],
      "metadata": {
        "id": "SZgtTo35ewzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reads every text file in a folder and returns a dictionary with the content as values and the filenames as keys.\n",
        "def load_text_files(folder_path):\n",
        "    data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                data[doc_id] = file.read()\n",
        "                doc_id_to_filename[doc_id] = filename\n",
        "                logging.info(f\"Loaded file: {filename} with doc_id: {doc_id}\")\n",
        "                doc_id += 1\n",
        "    return data, doc_id_to_filename\n"
      ],
      "metadata": {
        "id": "koFixHHFVqwj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Cleaning text</h1>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_g0b36pTf2_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes special characters from text, tokenizes it, eliminates stop words, and lemmatizes it.\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [LEMMATIZER.lemmatize(word) for word in tokens if word not in STOPWORDS]\n",
        "    return cleaned_tokens\n"
      ],
      "metadata": {
        "id": "7Wc-ljSifnvc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Constructing Inverted Index</h1>"
      ],
      "metadata": {
        "id": "fqa7E-9Bg9da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Builds an inverted index from cleaned text data along with tracks term frequencies.\n",
        "def build_inverted_index(data):\n",
        "    inverted_index = defaultdict(set)\n",
        "    term_frequencies = Counter()  # Track term frequencies\n",
        "    for doc_id, content in data.items():\n",
        "        cleaned_tokens = clean_text(content)\n",
        "        for token in cleaned_tokens:\n",
        "            inverted_index[token].add(doc_id)\n",
        "            term_frequencies[token] += 1\n",
        "    return inverted_index, term_frequencies\n"
      ],
      "metadata": {
        "id": "Awu1ZxByg8jh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Boolean Query Processing</h1>"
      ],
      "metadata": {
        "id": "5Sxs-H1Mu3_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AND operation"
      ],
      "metadata": {
        "id": "WIVMuo7Eu-F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the inverted index, it performs an AND query on the terms.\n",
        "def boolean_and(terms, inverted_index):\n",
        "    result_set = inverted_index.get(terms[0], set())\n",
        "    for term in terms[1:]:\n",
        "        result_set = result_set.intersection(inverted_index.get(term, set()))\n",
        "    return result_set"
      ],
      "metadata": {
        "id": "KIB9QFZEu8SG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OR Operation"
      ],
      "metadata": {
        "id": "YQDax_Q7vpJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the inverted index, it performs an OR query on the terms.\n",
        "def boolean_or(terms, inverted_index):\n",
        "    result_set = set()\n",
        "    for term in terms:\n",
        "        result_set = result_set.union(inverted_index.get(term, set()))\n",
        "    return result_set"
      ],
      "metadata": {
        "id": "BXz3SX7Ih6zR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOT Operation"
      ],
      "metadata": {
        "id": "uWpZIrt7v2kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the inverted index, it performs an NOT query on the terms.\n",
        "def boolean_not(term, inverted_index, total_docs):\n",
        "    return set(range(total_docs)) - inverted_index.get(term, set())\n"
      ],
      "metadata": {
        "id": "GtrVVrWtv0wa"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Processor"
      ],
      "metadata": {
        "id": "anl4dJrbwEOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Processes boolean queries using the AND, OR, and NOT operations.\n",
        "def boolean_query(query, inverted_index, total_docs):\n",
        "    tokens = query.lower().split()\n",
        "    if 'and' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "        return boolean_and(terms, inverted_index)\n",
        "    elif 'or' in tokens:\n",
        "        terms = [term for term in tokens if term not in ['and', 'or', 'not']]\n",
        "        return boolean_or(terms, inverted_index)\n",
        "    elif 'not' in tokens:\n",
        "        return boolean_not(tokens[1], inverted_index, total_docs)\n",
        "    else:\n",
        "        return inverted_index.get(tokens[0], set())"
      ],
      "metadata": {
        "id": "bogbJDfmwAuF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Doc ids to filenames"
      ],
      "metadata": {
        "id": "fNeK1uR6w9Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converts doc_ids to filenames\n",
        "def convert_doc_ids_to_filenames(result_set, doc_id_to_filename):\n",
        "    return [doc_id_to_filename[doc_id] for doc_id in result_set if doc_id in doc_id_to_filename]\n"
      ],
      "metadata": {
        "id": "rEby6FfXxR6Q"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writing Query Results to File"
      ],
      "metadata": {
        "id": "Rv48BN7L0bzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_query_results(queries, inverted_index, doc_id_to_filename, total_docs):\n",
        "    # Define folder and file dynamically\n",
        "    folder_path = \"/content/drive/MyDrive/TECH 400 Information Retrieval/result\"\n",
        "    results_file_path = os.path.join(folder_path, \"nisha_results.txt\")\n",
        "\n",
        "    with open(results_file_path, \"w\") as result_file:\n",
        "        for query in queries:\n",
        "            result_set = boolean_query(query, inverted_index, total_docs)\n",
        "            result_filenames = convert_doc_ids_to_filenames(result_set, doc_id_to_filename)\n",
        "            result_str = f\"Results for '{query}': {result_filenames}\\n\"\n",
        "            print(result_str)\n",
        "            result_file.write(result_str)\n"
      ],
      "metadata": {
        "id": "KuIOhCxa0g3Y"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main function"
      ],
      "metadata": {
        "id": "hCapDInDxbTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Define folder path (for the uploaded files)\n",
        "    folder_path = '/content/drive/MyDrive/TECH 400 Information Retrieval/Docs'\n",
        "\n",
        "    # Load text files\n",
        "    data, doc_id_to_filename = load_text_files(folder_path)\n",
        "\n",
        "    # Build inverted index and term frequencies\n",
        "    inverted_index, term_frequencies = build_inverted_index(data)\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"Fitness AND NOT exercise\",\n",
        "        \"Renewable OR reforestation\",\n",
        "        \"learning AND online\"\n",
        "    ]\n",
        "\n",
        "    # Process each query and display the results\n",
        "    for query in queries:\n",
        "        result_set = boolean_query(query, inverted_index, len(data))\n",
        "        result_filenames = convert_doc_ids_to_filenames(result_set, doc_id_to_filename)\n",
        "        print(f\"Results for '{query}': {result_filenames}\")\n",
        "\n",
        "        write_query_results(queries, inverted_index, doc_id_to_filename, len(data))\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZqCMLhTxamH",
        "outputId": "4984f060-4a85-4cae-d27f-c2b6633661b6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for 'Fitness AND NOT exercise': ['Balanced diet.txt', 'Sport and Fitness.txt']\n",
            "Results for 'Fitness AND NOT exercise': ['Balanced diet.txt', 'Sport and Fitness.txt']\n",
            "\n",
            "Results for 'Renewable OR reforestation': ['Climate Change and Environment.txt']\n",
            "\n",
            "Results for 'learning AND online': ['Education and Learning.txt']\n",
            "\n",
            "Results for 'Renewable OR reforestation': ['Climate Change and Environment.txt']\n",
            "Results for 'Fitness AND NOT exercise': ['Balanced diet.txt', 'Sport and Fitness.txt']\n",
            "\n",
            "Results for 'Renewable OR reforestation': ['Climate Change and Environment.txt']\n",
            "\n",
            "Results for 'learning AND online': ['Education and Learning.txt']\n",
            "\n",
            "Results for 'learning AND online': ['Education and Learning.txt']\n",
            "Results for 'Fitness AND NOT exercise': ['Balanced diet.txt', 'Sport and Fitness.txt']\n",
            "\n",
            "Results for 'Renewable OR reforestation': ['Climate Change and Environment.txt']\n",
            "\n",
            "Results for 'learning AND online': ['Education and Learning.txt']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kU_ziRmf1LKx"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}